 * Languages and Softwares: Python, MySQL and Microsoft Excel. 
 (Currently learning Tableau)
 * My Skills: Exploratory Data Analysis, Data Cleaning, Data Visualisation, Web scraping

# [Project-1: Exploratory Data Analysis on India's Statewise Covid-19 Data in Python](https://github.com/jatin-kohar/EDA-India-Covid19-Data)
* In this project, I analysed the extent of Covid-19 spread and its impact on all Indian states and UTs using Python. Further, I attempted to draw conclusions as to the reason why some states were hit harder as compared to others. I individually inspected the death count, death ratio, discharge ratio, active cases and active ratio to reach my conclusions.
* Libraries Used: NumPy, Pandas, Matplotlib, Plotly
* Skills used: Data Exploration, Interactive visualistions

  ### Summary of Findings:

  - Maharashtra has been the most affected state in the country with the highest number of total and active cases. This alongside a high death rate represents a dire situation of Covid infections in the state.
  - Kerala and Andhra Pradesh are outliers in the Deaths vs Total cases trend. Furthermore, the active ratio of Andhra Pradesh is below the national average Active ratio of 2.58%. This points an efficient healthcare system in the state.
  - The Northeastern states have the lowest discharge ratio. In conjunction to this, the region also has the highest Active ratio. One possible conclusion could be that the outbreak is still spreading in the region and the healthcare facilities need improvement to enhance the discharge ratio.

![Active-total cases](https://user-images.githubusercontent.com/84448617/127274124-8ec809da-dc8c-48d1-ac77-964d1bc03c22.png)
![newplot](https://user-images.githubusercontent.com/84448617/127274294-753867ac-c94d-47b3-a326-aa68c2afe4f7.png)

# [Project-2: Forecasting Tata Motors stock price using Monte Carlo Simulation in Python](https://github.com/jatin-kohar/Forecasting-Stock-Price)
* In this project, I created a forecast for the stock price of Tata Motors stock for the upcoming 1000 days using a Monte Carlo Simulation on the price data from the period 1-1-2007 till present.
* Libraries Used: NumPy, Pandas, Pandas_datareader, scipy, Matplotlib
* Skills Used: Extracting data from web, Handling DataFrames, Data Visualisation, Monte Carlo Simulation

![price projections](https://user-images.githubusercontent.com/84448617/127276397-b64b1cae-b3c7-42f7-af27-d13907da3da7.png)
![Screen Shot 2021-07-28 at 12 12 48 PM](https://user-images.githubusercontent.com/84448617/127276422-386c6649-bb14-4ac9-966c-46e641325333.png)

# [Project-3: Data Exploration on Global Covid-19 Deaths and Vaccination datasets in SQL](https://github.com/jatin-kohar/SQLProjects)
* In this project, I performed exploratory data analysis on global Covid-19 datasets using SQL. I further prepared the data for visualisation by creating views from the cleaned and analysed data.
* Skills used: Joins, CTE's, Temp Tables, Windows Functions, Aggregate Functions, Creating Views, Converting Data Types

# [Project-4: Data Cleaning on a Housing dataset using SQL](https://github.com/jatin-kohar/SQLProjects)
* The purpose of this mini-project is to showcase my data cleaning skills on a large housing dataset using SQL. It involves clearing duplicate fields, dropping empty datapoints, separating data from a column to our desired format, deleting usued columns and standardising datetime format.
* Skills used: Joins, Editing Tables, Removing duplicate values

# [Project-5: Scraping Data from a website using Python](https://github.com/jatin-kohar/DataScraping)
* This project involves scraping data from a website using the BeautifulSoup library of Python. The extracted data is formatted and printed in the notebook for further use. The program requires the user to input a skills that he is not familiar with so that the mentioned skills can be filtered away from our results. The posts are also filtered to show only the jobs that were listed a few days ago.
* Additionally, each entry from the data has been stored in separate text files that refresh automatically every 10 minutes to ensure that any changes made on the website are captured in our scraped data.
* Libraries Used: BeautifulSoup, requests, time
